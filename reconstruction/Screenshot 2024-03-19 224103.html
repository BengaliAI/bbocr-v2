<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0" name="viewport"/>
    <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
    <title>
      Document
    </title>
    <link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/css/bootstrap.min.css" integrity="sha384-KK94CHFLLe+nY2dmCWGMq91rCGa5gtU4mk92HdvYe+M/SXH301p5ILy+dN9+nJOZ" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet"/>
    <!--Custom style-->
    <style>
      #root > * {
          position: fixed;
      }

	  .paragraph-container {
			overflow: hidden;
		}
		
        .paragraph-content {
            font-size: 1px;  /* Set the initial font size */
        }
    </style>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js">
    </script>
    <script>
      $(document).ready(function() {
            $('.paragraph-container').each(function() {
                var containerHeight = $(this).height();
                var contentHeight = $(this).find('.paragraph-content').height();
                var fontSize = parseInt($(this).find('.paragraph-content').css('font-size'));
				var $paragraphContent = $(this).find('.paragraph-content');
                
				
				if (contentHeight < containerHeight && $paragraphContent.text().trim() !== '')  {
					while (contentHeight < containerHeight) {
                        fontSize += 1;
                        $(this).find('.paragraph-content').css('font-size', fontSize + 'px');
                        contentHeight = $(this).find('.paragraph-content').height();
						console.log('Content Height:', contentHeight);
						console.log('Container Height:', containerHeight);
                    }
					fontSize -= 1;
                    $(this).find('.paragraph-content').css('font-size', fontSize + 'px');
					contentHeight = $(this).find('.paragraph-content').height();
					console.log('Content Height:', contentHeight);
					console.log('Container Height:', containerHeight);
					
					while (contentHeight > 0.9*containerHeight){
						fontSize -= 1;
						$(this).find('.paragraph-content').css('font-size', fontSize + 'px');
						contentHeight = $(this).find('.paragraph-content').height();
						console.log('Content Height:', contentHeight);
						console.log('Container Height:', containerHeight);
					}
				}
            });
        });
    </script>
  </head>
  <body>
    <div id="root" style="display: flex; position: relative">
      <!--Elements Goes Here-->
      <div style="top: 23.33356132803037vh; left: 8.793380371709489vw; height: 24.492212044176203vh; width: 39.65069264511744vw;">
        <p>
          Recently Transformer and Convolution neural network (CNN)based models have shown promising results in Automaticnctworks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best ofboth worlds by studying how to combine convolution neuralnetworks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient wayTo this regard, we propose the convolution-augmented transsignificantly outperforms the previous Transformer and CNNwidely used LibriSpeech benchmark, our model achieves WERof 2.1%/4.3% without using a language model and 1.9%/3.9%with an external language model on tes/testother. We alsoobserve competitive performance of 2.7%/6.3% with a smallmodel of only 10M parameters.
        </p>
      </div>
      <div style="top: 54.0665441705275vh; left: 8.912169948351789vw; height: 15.161665095839382vh; width: 39.66917059610952vw;">
        <p>
          years. Recurrent neural networks (RNNs) have been the defacto choice for ASR [23]] as they can model the temporaldependencies in the audio sequences effectively 5j. Recently,the Transformer architecture based on self attention  ) hasenjoyed widespread adoption for modeling sequences due to itsability to capture long distance intcractions and the high train-cessful for ASR [8] [10, 1, [2], which capture local contex]progressively via a local receptive field layer by layer.
        </p>
      </div>
      <div style="top: 69.18783853220386vh; left: 8.89160906682249vw; height: 20.264381586119185vh; width: 39.63102843391639vw;">
        <p>
          However, models with self-attention or convolutions eachhas its limitations. While Transformers are good at modelinggrained local feature patterns. Convolution neural networks(CNNs), on the other hand, exploit local information and areused as the de facto computational block in vision. Thcy learnshared position-based kerncls over a local window which maintain translation equivariance and are able to capture features likeedges and shapes. One limitation of using local conncctivity isthat you need many more layers or paramelers to capture globalinformation. To combat this issue, contemporary work ContextNet [10] adopts the squeeze-and-excitation module [3] ineach residual block to capture longer context. However, it is stillimited in capturing dynamic global context as it only applies aglobal averaging over the cntire scquence.
        </p>
      </div>
      <div style="top: 71.6985539872517vh; left: 51.83267100311318vw; height: 14.952353544013441vh; width: 39.567506268225884vw;">
        <p>
          In this work, we study how to organically combine convolutions with self attention in ASR models. We hypothesizethat both global and local interactions are important for bcingparamcter efficicnt. To achicve this, we propose a novel combi-nation of sclf-attention and convolution will achicve the best ofboth worlds - self-attention learns the global interaction whilstcal correlations. Inspired by Wu et al. [Z] 18], we introducea novel combination of self-attention and convolution, sand-wiched between a pair feed forward modules, as illustrated inFig1
        </p>
      </div>
      <div style="top: 59.209134597186896vh; left: 51.82843009881384vw; height: 12.446517352910003vh; width: 39.54430829080627vw;">
        <p>
          self-attention improves over using them individually [14]. Together, they are able to learn both position-wise local features,and use content-based global interactions. Concurrently, paperslike [15, [16] have augmented self-attention with relative posi-put into two branchcs: sclf attcntion and convolution; and concatcnating thcir outputs. Thcir work targcted mobile applications and showed improvements in machine translation tasks.
        </p>
      </div>
      <div style="top: 49.392774123554084vh; left: 51.857200516728405vw; height: 7.057364412056377vh; width: 39.56582837868458vw;">
        <p>
          Figure 1: Conformer encoder model architecture. Conformercomprises of two macaron-like feed-forward layers with half.step residual connections sandwiching the multi-headed self-attention and convolution modules. This is followed by a postlayernorm.
        </p>
      </div>
      <div style="top: 86.59847170807595vh; left: 51.82276914579721vw; height: 4.46662607119065vh; width: 39.59805848168545vw;">
        <p>
          Our proposed model, named Conformer, achieves state-of-the-art results on LibriSpeech, outperforming the previous bestpublished Transformer Transducer [] by 15% relative improve
        </p>
      </div>
      <div style="top: 13.809207058692163vh; left: 43.64728314278255vw; height: 2.6919877991195778vh; width: 12.112030375650223vw;">
        <p>
          Google Inc.
        </p>
      </div>
      <div style="top: 15.790912716887718vh; left: 10.233725395058322vw; height: 3.344651155693587vh; width: 79.15598615571801vw;">
        <p>
          [anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,yonghui, rpangl@google.com
        </p>
      </div>
      <div style="top: 53.19161821705426vh; left: 1.9366043061957532vw; height: 30.905235349669947vh; width: 4.69528200647419vw;">
        <p>
        </p>
      </div>
      <div style="top: 38.90369917995246vh; left: 53.75658302259024vw; height: 0.8387366006540664vh; width: 3.768659209242986vw;">
        <p>
        </p>
      </div>
      <div style="top: 30.590867626574614vh; left: 53.52552685731597vw; height: 0.8157715316890746vh; width: 3.844064180968243vw;">
        <p>
        </p>
      </div>
      <div style="top: 36.2971963808518vh; left: 58.2020590013694vw; height: 2.3743953261264528vh; width: 5.332456110104045vw;">
        <p>
        </p>
      </div>
      <div style="top: 25.312438491703006vh; left: 55.23384931258867vw; height: 18.72865543809048vh; width: 9.938505348311399vw;">
        <p>
        </p>
        <img src="image/\Screenshot 2024-03-19 224103_20.png" style="width: 100%; height: 100%; object-fit: fill;"/>
      </div>
      <div style="top: 89.28372287010961vh; left: 11.206375064344755vw; height: 2.422982149345929vh; width: 37.32610656815249vw;">
        <p>
          Recent works have shown that combining convolution and
        </p>
      </div>
      <div style="top: 5.059028965558192vh; left: 23.00947076285171vw; height: 2.4980090385259577vh; width: 64.94378261878842vw;">
        <p>
        </p>
      </div>
      <div style="top: 4.7518593396327296vh; left: 10.375533037799004vw; height: 2.967346176620602vh; width: 78.75451356134981vw;">
        <p>
          ner for Speech Recognition
        </p>
      </div>
      <div style="top: 42.83279596373092vh; left: 53.66208243640644vw; height: 0.9537955587224423vh; width: 4.099830733271603vw;">
        <p>
        </p>
      </div>
      <div style="top: 32.512715066126155vh; left: 59.3242399930052vw; height: 0.9312681449476112vh; width: 2.9287292557436118vw;">
        <p>
        </p>
      </div>
      <div style="top: 34.51886435811834vh; left: 53.44125908831179vw; height: 0.9032700412957229vh; width: 3.9961402383157374vw;">
        <p>
        </p>
      </div>
    </div>
  </body>
</html>

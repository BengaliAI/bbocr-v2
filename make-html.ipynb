{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jumpstart? How?\n",
    "\n",
    "- Create a folder called \"reconstruction\"\n",
    "- Create another folder called \"templates\" inside reconstruction\n",
    "- Populate with appropriate HTML files (kinda vague, but can't wrap everything in this notebook now. Just ask me aka Istiak Shihab). Also I will provide you a snapshot of my current directory structure. Just copy over the files needed. I guess.\n",
    "- Create another folder called \"img_src\" inside reconstruction\n",
    "- Create another folder called \"image\" inside reconstruction\n",
    "- Create another folder called \"html_output\" inside reconstruction\n",
    "- Now get out of reconstruction folder and Create YET another folder called \"image\". This is where you will keep your PNG images to run inference on.\n",
    "- Make sure the folder structure is like this: image/   best.pt    make-html.ipynb  environment.yml    reconstruction/...\n",
    "- Run this command `conda env create -f environment.yml`\n",
    "- You should be good to go? Hopefully.\n",
    "- Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from apsisnet import ApsisNet\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.formatter import HTMLFormatter\n",
    "from ultralytics import YOLO\n",
    "from paddleocr import PaddleOCR\n",
    "from pathlib import Path\n",
    "from shapely.geometry import box\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to YOLO Model. Currently it assumes that YOLO is in it's root directory, i.e: where the notebook is, and it's name is \"best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model_weight_path = \"best.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Line Segmentation, Word Segmentation, OCR, and Layout Detection (YOLO) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/04/23 23:44:41] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\USER/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\USER/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\USER\\\\anaconda3\\\\envs\\\\bbocr-env-v2\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='C:\\\\Users\\\\USER/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/04/23 23:44:44] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\USER/.paddleocr/whl\\\\det\\\\ml\\\\Multilingual_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\USER/.paddleocr/whl\\\\rec\\\\arabic\\\\arabic_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\USER\\\\anaconda3\\\\envs\\\\bbocr-env-v2\\\\lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\dict\\\\arabic_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir='C:\\\\Users\\\\USER/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ar', det=True, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "line = PaddleOCR(use_angle_cls=False, lang=\"en\", use_gpu=True)\n",
    "word = PaddleOCR(use_angle_cls=False, lang=\"ar\", use_gpu=True)\n",
    "# ocr = PaddleOCR(use_angle_cls=True, lang=\"en\")\n",
    "ocr = ApsisNet()\n",
    "model = YOLO(yolo_model_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference helper functions for Line, Word segmentation and Word Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_segmentation(image):\n",
    "    result_line = line.ocr(image, rec=False, cls=False)[0]\n",
    "    return result_line\n",
    "\n",
    "\n",
    "def word_segmentation(image):\n",
    "    result_word = word.ocr(image, rec=False, cls=False)\n",
    "    return result_word\n",
    "\n",
    "def recognize_word(cropped_words):\n",
    "    # texts = ocr.ocr(image, det=False, rec=True, cls=True)[0][0][0]\n",
    "    texts = ocr.infer(cropped_words)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions for Inference run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padWordImage(img, pad_loc, pad_dim, pad_val):\n",
    "    \"\"\"\n",
    "    This function pads the image to the desired dimension\n",
    "    Args:\n",
    "    img: Image to be padded\n",
    "    pad_loc: Location of padding\n",
    "    pad_dim: Desired dimension of the padded image\n",
    "    pad_val: Value to be padded with\n",
    "    Returns:\n",
    "    img: Padded image\n",
    "    \"\"\"\n",
    "    if pad_loc == \"lr\":\n",
    "        h, w, d = img.shape\n",
    "        pad_width = pad_dim - w\n",
    "        pad = np.ones((h, pad_width, 3)) * pad_val\n",
    "        img = np.concatenate([img, pad], axis=1)\n",
    "    else:\n",
    "        h, w, d = img.shape\n",
    "        if h >= pad_dim:\n",
    "            return img\n",
    "        else:\n",
    "            pad_height = pad_dim - h\n",
    "            pad = np.ones((pad_height, w, 3)) * pad_val\n",
    "            img = np.concatenate([img, pad], axis=0)\n",
    "    return img.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctPadding(img, dim, pvalue=255):\n",
    "    \"\"\"\n",
    "    This function corrects the padding of the image\n",
    "    Args:\n",
    "    img: Image to be padded\n",
    "    dim: Desired dimension of the padded image\n",
    "    pvalue: Value to be padded with\n",
    "    Returns:\n",
    "    img: Padded image\n",
    "    mask: Mask of the padded image\n",
    "    \"\"\"\n",
    "    img_height, img_width = dim\n",
    "    mask = 0\n",
    "    h, w, d = img.shape\n",
    "    w_new = int(img_height * w / h)\n",
    "    img = cv2.resize(img, (w_new, img_height))\n",
    "    h, w, d = img.shape\n",
    "    if w > img_width:\n",
    "        h_new = int(img_width * h / w)\n",
    "        img = cv2.resize(img, (img_width, h_new))\n",
    "        img = padWordImage(img, pad_loc=\"tb\", pad_dim=img_height, pad_val=pvalue)\n",
    "        mask = img_width\n",
    "    elif w < img_width:\n",
    "        img = padWordImage(img, pad_loc=\"lr\", pad_dim=img_width, pad_val=pvalue)\n",
    "        mask = w\n",
    "    img = cv2.resize(img, (img_width, img_height))\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_horizontal_dilation(boxes, image):\n",
    "    \"\"\"\n",
    "    This function performs horizontal dilation on the word boxes\n",
    "    Args:\n",
    "    boxes: Word boxes\n",
    "    image: Image\n",
    "    Returns:\n",
    "    image: Image with horizontal dilation\n",
    "    \"\"\"\n",
    "    crops = []\n",
    "    length = len(boxes)\n",
    "    for i in range(len(boxes)):\n",
    "\n",
    "        if i + 1 < length:\n",
    "            [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = boxes[i]\n",
    "            [[x_min1, y_min1], [x_max1, y_min1], [x_max1, y_max1], [x_min1, y_max1]] = (\n",
    "                boxes[i + 1]\n",
    "            )\n",
    "\n",
    "            right_gap = x_min1 - x_max\n",
    "            if right_gap > 0:\n",
    "                x_max += right_gap // 2\n",
    "                x_min1 -= right_gap // 2\n",
    "\n",
    "                boxes[i][1][0], boxes[i][2][0] = x_max, x_max\n",
    "                boxes[i + 1][0][0], boxes[i + 1][3][0] = x_min1, x_min1\n",
    "\n",
    "                crop = image[int(y_min) : int(y_max), int(x_min) : int(x_max)]\n",
    "                h, w, d = crop.shape\n",
    "                if h != 0 and w != 0:\n",
    "                    crops.append(crop)\n",
    "\n",
    "    crop = image[\n",
    "        int(boxes[0][0][1]) : int(boxes[0][2][1]),\n",
    "        int(boxes[0][0][0]) : int(boxes[0][1][0]),\n",
    "    ]\n",
    "    h, w, d = crop.shape\n",
    "    if h != 0 and w != 0:\n",
    "        crops.append(crop)\n",
    "\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare HTMLGenerator Object. This Object takes care of fetching the template HTML file that is in \"reconstruction/templates/index.html\" file and populating this template using required class type layout and its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HtmlGenerator:\n",
    "    \"\"\"\n",
    "    This class generates the html file\n",
    "    \"\"\"\n",
    "    def __init__(self, filename=\"default\"):\n",
    "        \"\"\"\n",
    "        This function initializes the class\n",
    "        Args:\n",
    "        filename: Name of the html file\n",
    "        \"\"\"\n",
    "        with open(\"reconstruction/templates/index.html\", \"r\") as f:\n",
    "            index_template = f.read()\n",
    "\n",
    "        self.index_template = BeautifulSoup(index_template, \"html.parser\")\n",
    "        self.index_template_root_div = self.index_template.find(\"div\", {\"id\": \"root\"})\n",
    "        self.filename = filename\n",
    "\n",
    "    def read_html_template(self, template_name):\n",
    "        \"\"\"\n",
    "        This function reads the html template\n",
    "        Args:\n",
    "        template_name: Name of the template\n",
    "        Returns:\n",
    "        soup_template: Template\n",
    "        \"\"\"\n",
    "        with open(f\"reconstruction/templates/{template_name}.html\", \"r\") as f:\n",
    "            template = f.read()\n",
    "            soup_template = BeautifulSoup(template, \"html.parser\")\n",
    "            return soup_template\n",
    "\n",
    "    def get_styles(self, dict):\n",
    "        \"\"\"\n",
    "        This function gets the styles for the html elements\n",
    "        Args:\n",
    "        dict: Dictionary containing the styles\n",
    "        Returns:\n",
    "        styles: Styles for the html elements\n",
    "        \"\"\"\n",
    "        styles = f'top: {dict[\"top\"]}vh; left: {dict[\"left\"]}vw; height: {dict[\"elem_height\"]}vh; width: {dict[\"elem_width\"]}vw;'\n",
    "        return styles\n",
    "\n",
    "    def insert_paragraph(self, paragraph_info):\n",
    "        \"\"\"\n",
    "        This function inserts the paragraph into the html file\n",
    "        Args:\n",
    "        paragraph_info: Information about the paragraph\n",
    "        \"\"\"\n",
    "        paragraph_template = self.read_html_template(\"paragraph\")\n",
    "\n",
    "        p_tag = paragraph_template.find(\"p\")\n",
    "        text = paragraph_template.new_string(paragraph_info[\"text\"])\n",
    "        p_tag.append(text)\n",
    "\n",
    "        paragraph_div = paragraph_template.find(\"div\")\n",
    "        paragraph_div[\"style\"] = self.get_styles(paragraph_info)\n",
    "\n",
    "        self.index_template_root_div.append(paragraph_template)\n",
    "\n",
    "    def insert_text_box(self, text_box_info):\n",
    "        \"\"\"\n",
    "        This function inserts the text box into the html file\n",
    "        Args:\n",
    "        text_box_info: Information about the text box\n",
    "        \"\"\"\n",
    "        text_box_template = self.read_html_template(\"text_box\")\n",
    "\n",
    "        p_tag = text_box_template.find(\"p\")\n",
    "        text = text_box_template.new_string(text_box_info[\"text\"])\n",
    "        p_tag.append(text)\n",
    "\n",
    "        text_box_div = text_box_template.find(\"div\")\n",
    "        text_box_div[\"style\"] = self.get_styles(text_box_info)\n",
    "\n",
    "        self.index_template_root_div.append(text_box_template)\n",
    "\n",
    "    def insert_image(self, img_info):\n",
    "        \"\"\"\n",
    "        This function inserts the image into the html file\n",
    "        Args:\n",
    "        img_info: Information about the image\n",
    "        \"\"\"\n",
    "        image_template = self.read_html_template(\"image\")\n",
    "\n",
    "        img_div = image_template.find(\"div\")\n",
    "        img_div[\"style\"] = self.get_styles(img_info)\n",
    "\n",
    "        img_tag = image_template.new_tag(\"img\")\n",
    "        img_tag[\"src\"] = img_info[\"img_src\"]\n",
    "\n",
    "        img_style = \"width: 100%; height: 100%; object-fit: fill;\"\n",
    "        img_tag[\"style\"] = img_style\n",
    "\n",
    "        img_div.append(img_tag)\n",
    "\n",
    "        self.index_template_root_div.append(image_template)\n",
    "\n",
    "    def create_html_file(self):\n",
    "        \"\"\"\n",
    "        This function creates the html file\n",
    "        \"\"\"\n",
    "        global img_src_save_dir\n",
    "        html_path = Path(img_src_save_dir).parent\n",
    "        with open(html_path / f\"reconstruction/{self.filename}.html\", \"w\") as f:\n",
    "            f.write(\n",
    "                str(self.index_template.prettify(formatter=HTMLFormatter(indent=2)))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to initialize HTMLGenerator and passing element where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html(detected_elements_info, file_name):\n",
    "    \"\"\"\n",
    "    This function generates the html file\n",
    "    Args:\n",
    "    detected_elements_info: Information about the detected elements\n",
    "    file_name: Name of the file\n",
    "    \"\"\"\n",
    "    file_name, extension = file_name.split(\".\")\n",
    "\n",
    "    gen = HtmlGenerator(file_name)\n",
    "\n",
    "    for element_info in detected_elements_info:\n",
    "\n",
    "        if element_info[\"class\"] == \"paragraph\":\n",
    "            gen.insert_paragraph(element_info)\n",
    "\n",
    "        elif element_info[\"class\"] == \"text_box\":\n",
    "            gen.insert_text_box(element_info)\n",
    "\n",
    "        elif element_info[\"class\"] == \"image\":\n",
    "            gen.insert_image(element_info)\n",
    "\n",
    "    gen.create_html_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotate_crop_image(self, img: np.ndarray, points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get a rotated and cropped image.\n",
    "\n",
    "        Args:\n",
    "            img (np.ndarray): Input image.\n",
    "            points (np.ndarray): np.ndarray of four points defining the region to crop.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Rotated and cropped image.\n",
    "        \"\"\"    # Use Green's theory to judge clockwise or counterclockwise\n",
    "        # author: biyanhua\n",
    "        d = 0.0\n",
    "        for index in range(-1, 3):\n",
    "            d += -0.5 * (points[index + 1][1] + points[index][1]) * (\n",
    "                        points[index + 1][0] - points[index][0])\n",
    "        if d < 0: # counterclockwise\n",
    "            tmp = np.array(points)\n",
    "            points[1], points[3] = tmp[3], tmp[1]\n",
    "\n",
    "        img_crop_width = int(\n",
    "            max(\n",
    "                np.linalg.norm(points[0] - points[1]),\n",
    "                np.linalg.norm(points[2] - points[3])))\n",
    "        img_crop_height = int(\n",
    "            max(\n",
    "                np.linalg.norm(points[0] - points[3]),\n",
    "                np.linalg.norm(points[1] - points[2])))\n",
    "        pts_std = np.float32([[0, 0], [img_crop_width, 0],\n",
    "                            [img_crop_width, img_crop_height],\n",
    "                            [0, img_crop_height]])\n",
    "        M = cv2.getPerspectiveTransform(points, pts_std)\n",
    "        dst_img = cv2.warpPerspective(\n",
    "            img,\n",
    "            M, (img_crop_width, img_crop_height),\n",
    "            borderMode=cv2.BORDER_REPLICATE,\n",
    "            flags=cv2.INTER_CUBIC)\n",
    "        dst_img_height, dst_img_width = dst_img.shape[0:2]\n",
    "        if dst_img_height * 1.0 / dst_img_width >= 1.5:\n",
    "            dst_img = np.rot90(dst_img)\n",
    "        return dst_img\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to get proper coordinate information and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_coordinates(xyxy_tensor, height, width):\n",
    "    \"\"\"\n",
    "    This function gets the normalized coordinates\n",
    "    Args:\n",
    "    xyxy_tensor: Tensor containing the coordinates\n",
    "    height: Height of the image\n",
    "    width: Width of the image\n",
    "    Returns:\n",
    "    coordinates: Normalized coordinates\n",
    "    \"\"\"\n",
    "    x_min = xyxy_tensor[0][0].item() / width\n",
    "    y_min = xyxy_tensor[0][1].item() / height\n",
    "    x_max = xyxy_tensor[0][2].item() / width\n",
    "    y_max = xyxy_tensor[0][3].item() / height\n",
    "\n",
    "    coordinates = [x_min, y_min, x_max, y_max]\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def get_original_coordinates(normalized_coordinates, image_width, image_height):\n",
    "    \"\"\"\n",
    "    This function gets the original coordinates\n",
    "    Args:\n",
    "    normalized_coordinates: Normalized coordinates\n",
    "    image_width: Width of the image\n",
    "    image_height: Height of the image\n",
    "    Returns:\n",
    "    orig_coordinates: Original coordinates\n",
    "    \"\"\"\n",
    "    orig_coordinates = [None] * 4\n",
    "\n",
    "    orig_coordinates[0] = math.floor(normalized_coordinates[0] * image_width)\n",
    "    orig_coordinates[1] = math.floor(normalized_coordinates[1] * image_height)\n",
    "    orig_coordinates[2] = math.ceil(normalized_coordinates[2] * image_width)\n",
    "    orig_coordinates[3] = math.ceil(normalized_coordinates[3] * image_height)\n",
    "\n",
    "    return orig_coordinates\n",
    "\n",
    "\n",
    "def get_coordinates_from_segmentation(result_word):\n",
    "    \"\"\"\n",
    "    This function gets the coordinates from the segmentation\n",
    "    Args:\n",
    "    result_word: Result of the word segmentation\n",
    "    Returns:\n",
    "    words_xyxy: Coordinates of the words\n",
    "    \"\"\"\n",
    "    words_xyxy = []\n",
    "    if result_word != None:\n",
    "        for i in range(len(result_word)):\n",
    "            [[x_min, y_min], [x_max, y_min], [x_max, y_max], [x_min, y_max]] = (\n",
    "                result_word[i]\n",
    "            )\n",
    "            words_xyxy.append([int(x_min), int(y_min), int(x_max), int(y_max)])\n",
    "\n",
    "    return words_xyxy\n",
    "\n",
    "def top_bottom_padding(cropped_text_region):\n",
    "    \"\"\"\n",
    "    This function adds top and bottom padding to the cropped text region\n",
    "    Args:\n",
    "    cropped_text_region: Cropped text region\n",
    "    Returns:\n",
    "    padded_image: Padded image\n",
    "    \"\"\"\n",
    "    h, w = cropped_text_region.shape[:2]\n",
    "    padded_height = int(h * 1.5)\n",
    "    padded_width = w\n",
    "\n",
    "    padded_image = np.ones((padded_height, padded_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "    top_padding = (h * 2 - h) // 2\n",
    "    bottom_padding = top_padding + h\n",
    "\n",
    "    padded_image[top_padding:bottom_padding, :] = cropped_text_region\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes care of all the inference, that is, Layout Detection at first (`res = model(image)[0]`), then line segmentation, word segmentation and finally returning all of them with appropriate coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotate_crop_image( img: np.ndarray, points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get a rotated and cropped image.\n",
    "\n",
    "        Args:\n",
    "            img (np.ndarray): Input image.\n",
    "            points (np.ndarray): np.ndarray of four points defining the region to crop.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Rotated and cropped image.\n",
    "        \"\"\"    # Use Green's theory to judge clockwise or counterclockwise\n",
    "        # author: biyanhua\n",
    "        d = 0.0\n",
    "        for index in range(-1, 3):\n",
    "            d += -0.5 * (points[index + 1][1] + points[index][1]) * (\n",
    "                        points[index + 1][0] - points[index][0])\n",
    "        if d < 0: # counterclockwise\n",
    "            tmp = np.array(points)\n",
    "            points[1], points[3] = tmp[3], tmp[1]\n",
    "\n",
    "        img_crop_width = int(\n",
    "            max(\n",
    "                np.linalg.norm(points[0] - points[1]),\n",
    "                np.linalg.norm(points[2] - points[3])))\n",
    "        img_crop_height = int(\n",
    "            max(\n",
    "                np.linalg.norm(points[0] - points[3]),\n",
    "                np.linalg.norm(points[1] - points[2])))\n",
    "        pts_std = np.float32([[0, 0], [img_crop_width, 0],\n",
    "                            [img_crop_width, img_crop_height],\n",
    "                            [0, img_crop_height]])\n",
    "        M = cv2.getPerspectiveTransform(points, pts_std)\n",
    "        dst_img = cv2.warpPerspective(\n",
    "            img,\n",
    "            M, (img_crop_width, img_crop_height),\n",
    "            borderMode=cv2.BORDER_REPLICATE,\n",
    "            flags=cv2.INTER_CUBIC)\n",
    "        dst_img_height, dst_img_width = dst_img.shape[0:2]\n",
    "        if dst_img_height * 1.0 / dst_img_width >= 1.5:\n",
    "            dst_img = np.rot90(dst_img)\n",
    "        return dst_img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "names = {0: \"paragraph\", 1: \"text_box\", 2: \"image\", 3: \"table\"}\n",
    "\n",
    "# def run_inference(image_path, file_name, img_src_save_directory):\n",
    "#     \"\"\"\n",
    "#     This function runs the inference\n",
    "#     Args:\n",
    "#     image_path: Path of the image\n",
    "#     file_name: Name of the file\n",
    "#     img_src_save_directory: Directory to save the image\n",
    "#     Returns:\n",
    "#     region_of_interests: Region of interests\n",
    "#     \"\"\"\n",
    "#     file_name, extension = file_name.split(\".\")\n",
    "#     image = cv2.imread(image_path)\n",
    "#     res = model(image)[0]\n",
    "#     res.save(f\"reconstruction/image/{file_name}.{extension}\")\n",
    "#     region_of_interests = []\n",
    "#     for i in range(len(res.boxes)):\n",
    "#         info_dict = {\n",
    "#             \"class\": None,\n",
    "#             \"coordinates\": None,\n",
    "#             \"left\": None,\n",
    "#             \"top\": None,\n",
    "#             \"elem_height\": None,\n",
    "#             \"elem_width\": None,\n",
    "#             \"img_height\": None,\n",
    "#             \"img_width\": None,\n",
    "#             \"text\": None,\n",
    "#             \"single-line\": False,\n",
    "#             \"img_src\": None,\n",
    "#         }\n",
    "#         cls = res.boxes[i].cls.item()\n",
    "#         img_height, img_width = res.boxes[i].orig_shape\n",
    "#         normalized_coordinates = get_normalized_coordinates(\n",
    "#             res.boxes[i].xyxy, img_height, img_width\n",
    "#         )\n",
    "#         if cls == 0:\n",
    "#             info_dict[\"class\"] = names[0]\n",
    "#         elif cls == 1:\n",
    "#             info_dict[\"class\"] = names[1]\n",
    "#         elif cls == 2:\n",
    "#             info_dict[\"class\"] = names[2]\n",
    "#         elif cls == 3:\n",
    "#             info_dict[\"class\"] = names[3]\n",
    "#         info_dict[\"coordinates\"] = normalized_coordinates\n",
    "#         info_dict[\"left\"], info_dict[\"top\"] = (\n",
    "#             normalized_coordinates[0] * 100,\n",
    "#             normalized_coordinates[1] * 100,\n",
    "#         )\n",
    "#         info_dict[\"img_height\"], info_dict[\"img_width\"] = img_height, img_width\n",
    "#         info_dict[\"elem_width\"] = (\n",
    "#             normalized_coordinates[2] - normalized_coordinates[0]\n",
    "#         ) * 100\n",
    "#         info_dict[\"elem_height\"] = (\n",
    "#             normalized_coordinates[3] - normalized_coordinates[1]\n",
    "#         ) * 100\n",
    "#         if info_dict[\"class\"] == \"paragraph\" or info_dict[\"class\"] == \"text_box\":\n",
    "#             x_min, y_min, x_max, y_max = get_original_coordinates(\n",
    "#                 normalized_coordinates, info_dict[\"img_width\"], info_dict[\"img_height\"]\n",
    "#             )\n",
    "#             cropped_text_region = image[y_min:y_max, x_min:x_max]\n",
    "\n",
    "#             cropped_text_region = top_bottom_padding(cropped_text_region)\n",
    "#             result_line = line_segmentation(cropped_text_region)\n",
    "#             line_coordinates = get_coordinates_from_segmentation(result_line)\n",
    "#             sorted_line_coordinates = sorted(line_coordinates, key=lambda x: x[1])\n",
    "#             text = []\n",
    "#             for i in range(len(sorted_line_coordinates)):\n",
    "#                 cropped_line_region = cropped_text_region[\n",
    "#                     sorted_line_coordinates[i][1] : sorted_line_coordinates[i][3],\n",
    "#                     sorted_line_coordinates[i][0] : sorted_line_coordinates[i][2],\n",
    "#                 ]\n",
    "#                 if len(sorted_line_coordinates) == 1:\n",
    "#                     info_dict[\"single-line\"] = True\n",
    "#                     info_dict[\"elem_height\"] *= 1.4\n",
    "#                 if len(cropped_line_region) != 0:\n",
    "#                     result_word = word_segmentation(cropped_line_region)[0]\n",
    "#                     if result_word != None:\n",
    "#                         if len(result_word) != 0:\n",
    "#                             sorted_result_word = sorted(result_word, key=lambda x: min(coord[0] for coord in x))\n",
    "#                             # From here\n",
    "#                             # print(result_word.shape)\n",
    "#                             # sorted_result_word = sorted(\n",
    "#                             #     result_word[0], key=lambda x: x[0]\n",
    "#                             # )\n",
    "#                             # print(sorted_result_word)\n",
    "#                             # print(np.array(result_word).shape)\n",
    "#                             # print(type(result_word))\n",
    "#                             # print(result_word)\n",
    "#                             # print(sorted_result_word)\n",
    "\n",
    "#                             # To here\n",
    "\n",
    "#                             if len(sorted_result_word) != 0:\n",
    "#                                 crops=[]\n",
    "#                                 for tmp_box in  sorted_result_word:\n",
    "#                                     print(np.array(tmp_box).shape)\n",
    "#                                     img_crop = get_rotate_crop_image(cropped_line_region,np.array(tmp_box,dtype=np.float32))\n",
    "#                                     crops.append(img_crop)\n",
    "#                                 words = recognize_word(crops)\n",
    "#                                 print(words)\n",
    "#                                 text += words\n",
    "#             text = \" \".join(text)\n",
    "#             info_dict[\"text\"] = text\n",
    "#         elif info_dict[\"class\"] == \"image\":\n",
    "#             x_min, y_min, x_max, y_max = get_original_coordinates(\n",
    "#                 normalized_coordinates, info_dict[\"img_width\"], info_dict[\"img_height\"]\n",
    "#             )\n",
    "#             cropped_image_region = image[y_min:y_max, x_min:x_max]\n",
    "#             src = f\"{img_src_save_directory}\\\\{file_name}_{i}.{extension}\"\n",
    "#             info_dict[\"img_src\"] = src\n",
    "#             cv2.imwrite(\n",
    "#                 f\"reconstruction/image/{file_name}_{i}.{extension}\",\n",
    "#                 cropped_image_region,\n",
    "#             )\n",
    "#         region_of_interests.append(info_dict)\n",
    "#     discard_elements = []\n",
    "#     for i, element in enumerate(region_of_interests):\n",
    "#         bb1 = box(\n",
    "#             element[\"coordinates\"][0],\n",
    "#             element[\"coordinates\"][1],\n",
    "#             element[\"coordinates\"][2],\n",
    "#             element[\"coordinates\"][3],\n",
    "#         )\n",
    "\n",
    "#         for j, other_element in enumerate(region_of_interests):\n",
    "#             if j > i:\n",
    "#                 bb2 = box(\n",
    "#                     other_element[\"coordinates\"][0],\n",
    "#                     other_element[\"coordinates\"][1],\n",
    "#                     other_element[\"coordinates\"][2],\n",
    "#                     other_element[\"coordinates\"][3],\n",
    "#                 )\n",
    "#                 intersection = bb1.intersection(bb2).area\n",
    "#                 if bb1.area < bb2.area:\n",
    "#                     iou = intersection / bb1.area\n",
    "#                     if iou > 0.5:\n",
    "#                         if i not in discard_elements:\n",
    "#                             discard_elements.append(i)\n",
    "#                 else:\n",
    "#                     iou = intersection / bb2.area\n",
    "#                     if iou > 0.5:\n",
    "#                         if j not in discard_elements:\n",
    "#                             discard_elements.append(j)\n",
    "#     items_deleted = 0\n",
    "#     for index in discard_elements:\n",
    "#         del region_of_interests[index - items_deleted]\n",
    "#         items_deleted += 1\n",
    "#     return region_of_interests\n",
    "\n",
    "def run_inference(image_path, file_name, img_src_save_directory):\n",
    "    \"\"\"\n",
    "    This function runs the inference\n",
    "    Args:\n",
    "    image_path: Path of the image\n",
    "    file_name: Name of the file\n",
    "    img_src_save_directory: Directory to save the image\n",
    "    Returns:\n",
    "    region_of_interests: Region of interests\n",
    "    \"\"\"\n",
    "    file_name, extension = file_name.split(\".\")\n",
    "    image = cv2.imread(image_path)\n",
<<<<<<< HEAD
    "\n",
    "    # Record the start time for segmentation\n",
    "    segmentation_start_time = time.time()\n",
    "\n",
    "    res = model(image)[0]\n",
    "    res.save(f\"reconstruction/image/{file_name}.{extension}\")\n",
    "\n",
    "    # Calculate segmentation runtime\n",
    "    segmentation_runtime = time.time() - segmentation_start_time\n",
    "\n",
=======
    "    # Start time record\n",
    "    segmentation_start_time = time.time();\n",
    "    res = model(image)[0]\n",
    "    res.save(f\"reconstruction/image/{file_name}.{extension}\")\n",
    "    # Calculate Segmentation runtime\n",
    "    segmentation_runtime = time.time() - segmentation_start_time\n",
>>>>>>> 5d906bf2cfeff7b0e9fa33c3dd377dad6dccaeaa
    "    region_of_interests = []\n",
    "    for i in range(len(res.boxes)):\n",
    "        info_dict = {\n",
    "            \"class\": None,\n",
    "            \"coordinates\": None,\n",
    "            \"left\": None,\n",
    "            \"top\": None,\n",
    "            \"elem_height\": None,\n",
    "            \"elem_width\": None,\n",
    "            \"img_height\": None,\n",
    "            \"img_width\": None,\n",
    "            \"text\": None,\n",
    "            \"single-line\": False,\n",
    "            \"img_src\": None,\n",
    "        }\n",
    "\n",
    "        # Your existing code...\n",
    "\n",
<<<<<<< HEAD
    "    return region_of_interests, segmentation_runtime\n"
=======
    "        for j, other_element in enumerate(region_of_interests):\n",
    "            if j > i:\n",
    "                bb2 = box(\n",
    "                    other_element[\"coordinates\"][0],\n",
    "                    other_element[\"coordinates\"][1],\n",
    "                    other_element[\"coordinates\"][2],\n",
    "                    other_element[\"coordinates\"][3],\n",
    "                )\n",
    "                intersection = bb1.intersection(bb2).area\n",
    "                if bb1.area < bb2.area:\n",
    "                    iou = intersection / bb1.area\n",
    "                    if iou > 0.5:\n",
    "                        if i not in discard_elements:\n",
    "                            discard_elements.append(i)\n",
    "                else:\n",
    "                    iou = intersection / bb2.area\n",
    "                    if iou > 0.5:\n",
    "                        if j not in discard_elements:\n",
    "                            discard_elements.append(j)\n",
    "    items_deleted = 0\n",
    "    for index in discard_elements:\n",
    "        del region_of_interests[index - items_deleted]\n",
    "        items_deleted += 1\n",
    "    return region_of_interests, segmentation_runtime"
>>>>>>> 5d906bf2cfeff7b0e9fa33c3dd377dad6dccaeaa
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a gigantic class, takes care of everything, that is, calling inference and passing it to HTML Geneation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "File name: India.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 896x960 1 paragraph, 14112.4ms\n",
      "Speed: 154.0ms preprocess, 14112.4ms inference, 105.0ms postprocess per image at shape (1, 3, 896, 960)\n",
      "----------------------------------------------------------------------------\n",
      "File name: Soviet.png\n",
      "\n",
      "0: 544x960 1 paragraph, 5337.1ms\n",
      "Speed: 11.2ms preprocess, 5337.1ms inference, 8.9ms postprocess per image at shape (1, 3, 544, 960)\n"
     ]
    }
   ],
   "source": [
    "# def reconstruct(directory, img_src_save_dir):\n",
    "#     \"\"\"\n",
    "#     This function reconstructs the image\n",
    "#     Args:\n",
    "#     directory: Directory containing the images\n",
    "#     img_src_save_dir: Directory to save the image\n",
    "#     \"\"\"\n",
    "#     directory = \"image/\"  # Replace with your test directory path\n",
    "#     img_src_save_dir = \"image/\"  # Replace with your image source directory path\n",
    "\n",
    "#     for file_name in os.listdir(directory):\n",
    "#         if os.path.isfile(os.path.join(directory, file_name)):\n",
    "\n",
    "#             file_path = directory + \"/\" + file_name\n",
    "\n",
    "#             print(\n",
    "#                 \"----------------------------------------------------------------------------\"\n",
    "#             )\n",
    "#             print(\"File name:\", file_name)\n",
    "\n",
    "#             start_time = time.time()\n",
    "#             roi = run_inference(\n",
    "#                 file_path, file_name, img_src_save_dir\n",
    "#             )\n",
    "#             print(\n",
    "#                 \"Execution Time for Layout Prediction and Text Recognition:\",\n",
    "#                 round(time.time() - start_time, 2),\n",
    "#                 \"seconds\",\n",
    "#             )\n",
    "\n",
    "#             start_time = time.time()\n",
    "#             print(roi)\n",
    "#             generate_html(roi, file_name)\n",
    "#             print(\n",
    "#                 \"Execution Time for Reconstruction:\",\n",
    "#                 round(time.time() - start_time, 2),\n",
    "#                 \"seconds\",\n",
    "#             )\n",
    "\n",
    "\n",
    "import pandas as pd  # Import Pandas library\n",
    "\n",
    "def reconstruct(directory, img_src_save_dir):\n",
    "    \"\"\"\n",
    "    This function reconstructs the image and saves runtime information to a spreadsheet\n",
    "    Args:\n",
    "    directory: Directory containing the images\n",
    "    img_src_save_dir: Directory to save the image\n",
    "    \"\"\"\n",
    "    # Create an empty list to store runtime information\n",
    "    runtime_info = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, file_name)):\n",
    "\n",
    "            file_path = directory + \"/\" + file_name\n",
    "\n",
    "            print(\n",
    "                \"----------------------------------------------------------------------------\"\n",
    "            )\n",
    "            print(\"File name:\", file_name)\n",
    "\n",
    "            # Record the start time\n",
    "            total_start_time = time.time()\n",
    "\n",
    "            start_time = time.time()\n",
    "            roi, segmentation_runtime = run_inference(\n",
    "                file_path, file_name, img_src_save_dir\n",
    "            )\n",
    "            layout_analysis_runtime = time.time() - start_time\n",
    "\n",
    "            start_time = time.time()\n",
    "            generate_html(roi, file_name)\n",
    "            reconstruction_runtime = time.time() - start_time\n",
    "\n",
    "            # Calculate total runtime\n",
    "            total_runtime = time.time() - total_start_time\n",
    "\n",
    "            # Append runtime information to the list\n",
    "            runtime_info.append({\n",
    "                'Filename': file_name,\n",
    "                'Total_runtime': total_runtime,\n",
    "                'Reconstruction_runtime': reconstruction_runtime,\n",
    "                'Layout_analysis_runtime': layout_analysis_runtime,\n",
    "                'Segmentation_runtime': segmentation_runtime\n",
    "            })\n",
    "\n",
    "    # Convert the list to a Pandas DataFrame\n",
    "    df = pd.DataFrame(runtime_info)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('runtime_info.csv', index=False)\n",
    "\n",
    "test_image_directory = \"image/\"\n",
    "img_src_save_dir = \"image/\"\n",
<<<<<<< HEAD
    "reconstruct(test_image_directory, img_src_save_dir)"
=======
    "reconstruct(test_image_directory, img_src_save_dir)\n"
>>>>>>> 5d906bf2cfeff7b0e9fa33c3dd377dad6dccaeaa
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify this to your need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "File name: India.png\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 896x960 1 paragraph, 6712.2ms\n",
      "Speed: 19.0ms preprocess, 6712.2ms inference, 47.7ms postprocess per image at shape (1, 3, 896, 960)\n",
      "----------------------------------------------------------------------------\n",
      "File name: Soviet.png\n",
      "\n",
      "0: 544x960 1 paragraph, 5032.1ms\n",
      "Speed: 17.0ms preprocess, 5032.1ms inference, 8.0ms postprocess per image at shape (1, 3, 544, 960)\n"
     ]
    }
   ],
   "source": [
    "test_image_directory = \"image/\"\n",
    "img_src_save_dir = \"image/\"\n",
    "reconstruct(test_image_directory, img_src_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.0, 6.0], [52.0, 11.0], [50.0, 39.0], [0.0, 34.0]], [[64.0, 11.0], [260.0, 6.0], [261.0, 36.0], [65.0, 39.0]], [[275.0, 8.0], [342.0, 12.0], [341.0, 38.0], [274.0, 35.0]], [[352.0, 12.0], [394.0, 12.0], [394.0, 35.0], [352.0, 35.0]], [[411.0, 11.0], [473.0, 10.0], [474.0, 34.0], [411.0, 35.0]], [[482.0, 10.0], [571.0, 10.0], [571.0, 32.0], [482.0, 32.0]], [[588.0, 0.0], [634.0, 3.0], [632.0, 37.0], [586.0, 33.0]], [[638.0, 2.0], [711.0, 2.0], [711.0, 34.0], [638.0, 34.0]], [[718.0, 4.0], [777.0, 4.0], [777.0, 31.0], [718.0, 31.0]], [[779.0, 8.0], [807.0, 8.0], [807.0, 34.0], [779.0, 34.0]]]\n"
     ]
    }
   ],
   "source": [
    "result_word = [[[352.0, 12.0], [394.0, 12.0], [394.0, 35.0], [352.0, 35.0]], [[411.0, 11.0], [473.0, 10.0], [474.0, 34.0], [411.0, 35.0]], [[275.0, 8.0], [342.0, 12.0], [341.0, 38.0], [274.0, 35.0]], [[64.0, 11.0], [260.0, 6.0], [261.0, 36.0], [65.0, 39.0]], [[482.0, 10.0], [571.0, 10.0], [571.0, 32.0], [482.0, 32.0]], [[1.0, 6.0], [52.0, 11.0], [50.0, 39.0], [0.0, 34.0]], [[779.0, 8.0], [807.0, 8.0], [807.0, 34.0], [779.0, 34.0]], [[718.0, 4.0], [777.0, 4.0], [777.0, 31.0], [718.0, 31.0]], [[638.0, 2.0], [711.0, 2.0], [711.0, 34.0], [638.0, 34.0]], [[588.0, 0.0], [634.0, 3.0], [632.0, 37.0], [586.0, 33.0]]]\n",
    "sorted_result_word = sorted(result_word, key=lambda x: x[0])\n",
    "print(sorted_result_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 6.0], [52.0, 11.0], [50.0, 39.0], [0.0, 34.0]],\n",
       " [[64.0, 11.0], [260.0, 6.0], [261.0, 36.0], [65.0, 39.0]],\n",
       " [[275.0, 8.0], [342.0, 12.0], [341.0, 38.0], [274.0, 35.0]],\n",
       " [[352.0, 12.0], [394.0, 12.0], [394.0, 35.0], [352.0, 35.0]],\n",
       " [[411.0, 11.0], [473.0, 10.0], [474.0, 34.0], [411.0, 35.0]],\n",
       " [[482.0, 10.0], [571.0, 10.0], [571.0, 32.0], [482.0, 32.0]],\n",
       " [[588.0, 0.0], [634.0, 3.0], [632.0, 37.0], [586.0, 33.0]],\n",
       " [[638.0, 2.0], [711.0, 2.0], [711.0, 34.0], [638.0, 34.0]],\n",
       " [[718.0, 4.0], [777.0, 4.0], [777.0, 31.0], [718.0, 31.0]],\n",
       " [[779.0, 8.0], [807.0, 8.0], [807.0, 34.0], [779.0, 34.0]]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_word = [[[352.0, 12.0], [394.0, 12.0], [394.0, 35.0], [352.0, 35.0]], \n",
    "               [[411.0, 11.0], [473.0, 10.0], [474.0, 34.0], [411.0, 35.0]], \n",
    "               [[275.0, 8.0], [342.0, 12.0], [341.0, 38.0], [274.0, 35.0]], \n",
    "               [[64.0, 11.0], [260.0, 6.0], [261.0, 36.0], [65.0, 39.0]], \n",
    "               [[482.0, 10.0], [571.0, 10.0], [571.0, 32.0], [482.0, 32.0]], \n",
    "               [[1.0, 6.0], [52.0, 11.0], [50.0, 39.0], [0.0, 34.0]], \n",
    "               [[779.0, 8.0], [807.0, 8.0], [807.0, 34.0], [779.0, 34.0]], \n",
    "               [[718.0, 4.0], [777.0, 4.0], [777.0, 31.0], [718.0, 31.0]], \n",
    "               [[638.0, 2.0], [711.0, 2.0], [711.0, 34.0], [638.0, 34.0]], \n",
    "               [[588.0, 0.0], [634.0, 3.0], [632.0, 37.0], [586.0, 33.0]]]\n",
    "\n",
    "# Sort the result_word list based on the leftmost coordinate of each textbox\n",
    "sorted_result_word = sorted(result_word, key=lambda x: min(coord[0] for coord in x))\n",
    "\n",
    "# Print the sorted result\n",
    "# for box in sorted_result_word:\n",
    "#     print(box)\n",
    "sorted_result_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_word[0][0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
